{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not infer format, so each element will be parsed individually\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1:  EXTRACT DATA FOR 2011 - 2013, 2016, and 2018 - 2019 FIRES \n",
    "\n",
    "# Define path to the source files\n",
    "file_2011 = 'Resources/CAL_FireStats/2011-wildfire-activity-stats.xlsx'\n",
    "file_2012 = 'Resources/CAL_FireStats/2012-wildfire-activity-stats.xlsx'\n",
    "file_2013 = 'Resources/CAL_FireStats/2013-wildfire-activity-stats.xlsx'\n",
    "file_2016 = 'Resources/CAL_FireStats/2016-wildfire-activity-stats.xlsx'\n",
    "file_2018 = 'Resources/CAL_FireStats/2018-wildfire-activity-stats.xlsx'\n",
    "file_2019 = 'Resources/CAL_FireStats/2019-wildfire-activity-stats.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sheets to be read (all these files contain needed information on the same pages)\n",
    "sheets = ['table_page_13', 'table_page_14', 'table_page_15', 'table_page_16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be extracted\n",
    "columns_to_extract = ['County', 'Fire Name', 'Start', 'Cont.', 'Total', \n",
    "                      'Dest.', 'Dam.', 'Fire', 'Civil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and process the data from the Excel files\n",
    "def load_wildfire_data(file_path, sheets, columns_to_extract):\n",
    "    # Initiate an empty Data Frame\n",
    "    extracted_data = pd.DataFrame()\n",
    "    # Set a 'for' loop to go through identified path, sheets, and columns\n",
    "    # skip the 1st row as it does not contain iseful information\n",
    "    for sheet_name in sheets:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1, usecols=columns_to_extract)\n",
    "        # Add the data to an existing collection of data, Index=True to re-number the rows after adding a new data\n",
    "        extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Start' and 'Cont.' columns to datetime format\n",
    "    extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
    "    extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Dest.', 'Dam.', 'Fire', and 'Civil' columns to integers and replace NaN values with zero\n",
    "    extracted_data['Dest.'] = pd.to_numeric(extracted_data['Dest.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Dam.'] = pd.to_numeric(extracted_data['Dam.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Fire'] = pd.to_numeric(extracted_data['Fire'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Civil'] = pd.to_numeric(extracted_data['Civil'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Rename columns\n",
    "    extracted_data = extracted_data.rename(columns={'Cont.': \"Contained\", 'Total': 'Acres', 'Dest.': 'Strux_Destr', 'Dam.': 'Strux_Dmgd', \n",
    "                                                    'Fire': 'Deaths_FF', 'Civil': 'Deaths_Civil'})\n",
    "    \n",
    "    # Create a new column 'Duration' that calculates the number of days between 'Start' and 'Contained'\n",
    "    extracted_data['Duration'] = (extracted_data['Contained'] - extracted_data['Start']).dt.days + 1\n",
    "\n",
    "    # Reorder columns\n",
    "    extracted_data = extracted_data[['County', 'Fire Name', 'Start', 'Contained', 'Acres', \n",
    "                                     'Strux_Destr', 'Strux_Dmgd', 'Deaths_FF', 'Deaths_Civil', 'Duration']]\n",
    "    # Return the processed data as a DataFrame so it can be used outside the function\n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for 2016, 2018, 2019 years\n",
    "fires_2011 = load_wildfire_data(file_2011, sheets, columns_to_extract)\n",
    "fires_2012 = load_wildfire_data(file_2012, sheets, columns_to_extract)\n",
    "fires_2013 = load_wildfire_data(file_2013, sheets, columns_to_extract)\n",
    "fires_2016 = load_wildfire_data(file_2016, sheets, columns_to_extract)\n",
    "fires_2018 = load_wildfire_data(file_2018, sheets, columns_to_extract)\n",
    "fires_2019 = load_wildfire_data(file_2019, sheets, columns_to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data from 2016, 2018, and 2019 into one DataFrame\n",
    "fires_2011_2012_2013_2016_2018_2019_data = pd.concat([fires_2011, fires_2012, fires_2013, fires_2016, fires_2018, fires_2019], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 Fire Data:\n",
      "  County  Fire Name      Start  Contained Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0   INYO  WINTERTON 2011-03-09 2011-03-09   395            0           0   \n",
      "1   INYO     CENTER 2011-03-18 2011-03-23   850           19           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       1.0  \n",
      "1          0             0       6.0  \n",
      "--------------------------------------\n",
      "\n",
      "Combined Fire Data (2011-2013,2016, 2018-2019):\n",
      "     County  Fire Name      Start  Contained Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0      INYO  WINTERTON 2011-03-09 2011-03-09   395            0           0   \n",
      "1      INYO     CENTER 2011-03-18 2011-03-23   850           19           0   \n",
      "2  MONTEREY       METZ 2011-05-12 2011-05-14   832            0           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       1.0  \n",
      "1          0             0       6.0  \n",
      "2          0             0       3.0  \n"
     ]
    }
   ],
   "source": [
    "# Display the first two rows of the 2011 and combined dataframes to check the results\n",
    "print(\"2011 Fire Data:\")\n",
    "print(fires_2011.head(2))\n",
    "print(\"--------------------------------------\")\n",
    "print(\"\\nCombined Fire Data (2011-2013,2016, 2018-2019):\")\n",
    "print(fires_2011_2012_2013_2016_2018_2019_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2011.to_csv('Outputs/2011_wildfire_data.csv', index=False)\n",
    "fires_2012.to_csv('Outputs/2012_wildfire_data.csv', index=False)\n",
    "fires_2013.to_csv('Outputs/2013_wildfire_data.csv', index=False)\n",
    "fires_2016.to_csv('Outputs/2016_wildfire_data.csv', index=False)\n",
    "fires_2018.to_csv('Outputs/2018_wildfire_data.csv', index=False)\n",
    "fires_2019.to_csv('Outputs/2019_wildfire_data.csv', index=False)\n",
    "fires_2011_2012_2013_2016_2018_2019_data.to_csv('Outputs/2011_2012_2013_2016_2018_2019_wildfire_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Fire Data:\n",
      "   County  Fire Name      Start  Contained  Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0  FRESNO      JAYNE 2017-04-20 2017-04-21  4,532            0           0   \n",
      "1  FRESNO  EL DORADO 2017-04-28 2017-04-28    750            0           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       2.0  \n",
      "1          0             0       1.0  \n"
     ]
    }
   ],
   "source": [
    "#  PART 2: EXTRACT DATA FOR 2017 FIRES:\n",
    "\n",
    "# Define path to the source file of fires 2017\n",
    "file_2017 = 'Resources/CAL_FireStats/2017-wildfire-activity-stats.xlsx'\n",
    "# Define the sheets to be read thta contain needed information (different number of pages than 2016, 2018-2019 files)\n",
    "sheets = ['table_page_13', 'table_page_14', 'table_page_15', 'table_page_16', 'table_page_17', 'table_page_18']\n",
    "# Define the columns to be extracted\n",
    "columns_to_extract = ['County', 'Fire Name', 'Start', 'Cont.', 'Total', \n",
    "                      'Dest.', 'Dam.', 'Fire', 'Civil']\n",
    "# Function to load and process the data from the given Excel file\n",
    "def load_wildfire_data(file_path, sheets, columns_to_extract):\n",
    "    extracted_data = pd.DataFrame()\n",
    "    \n",
    "    for sheet_name in sheets:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1, usecols=columns_to_extract)\n",
    "        extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Start' and 'Cont.' columns to datetime format\n",
    "    extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
    "    extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Dest.', 'Dam.', 'Fire', and 'Civil' columns to integers and replace NaN values with zero\n",
    "    extracted_data['Dest.'] = pd.to_numeric(extracted_data['Dest.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Dam.'] = pd.to_numeric(extracted_data['Dam.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Fire'] = pd.to_numeric(extracted_data['Fire'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Civil'] = pd.to_numeric(extracted_data['Civil'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Rename columns\n",
    "    extracted_data = extracted_data.rename(columns={'Cont.': \"Contained\", 'Total': 'Acres', 'Dest.': 'Strux_Destr', 'Dam.': 'Strux_Dmgd', \n",
    "                                                    'Fire': 'Deaths_FF', 'Civil': 'Deaths_Civil'})\n",
    "    \n",
    "    # Create a new column 'Duration' that calculates the number of days between 'Start' and 'Contained'\n",
    "    extracted_data['Duration'] = (extracted_data['Contained'] - extracted_data['Start']).dt.days + 1\n",
    "\n",
    "    # Reorder columns\n",
    "    extracted_data = extracted_data[['County', 'Fire Name', 'Start', 'Contained', 'Acres', \n",
    "                                     'Strux_Destr', 'Strux_Dmgd', 'Deaths_FF', 'Deaths_Civil', 'Duration']]\n",
    "    \n",
    "    return extracted_data\n",
    "# Load data for 2017\n",
    "fires_2017 = load_wildfire_data(file_2017, sheets, columns_to_extract)\n",
    "# Display the first two rows of the data frame\n",
    "print(\"2017 Fire Data:\")\n",
    "print(fires_2017.head(2))\n",
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2017.to_csv('Outputs/2017_wildfire_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014 Fire Data:\n",
      "      County Fire Name      Start  Contained Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0   HUMBOLDT       RED 2014-01-04 2014-01-12   333            0           0   \n",
      "1  RIVERSIDE    PIERCE 2014-03-15 2014-03-16   350            0           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       9.0  \n",
      "1          0             0       2.0  \n",
      "--------------------------------------------\n",
      "2015 Fire Data:\n",
      "      County Fire Name      Start  Contained  Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0       INYO     ROUND 2015-02-06 2015-02-13  7,000           43           5   \n",
      "1  RIVERSIDE   HIGHWAY 2015-04-18 2015-04-24  1,049            0           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       8.0  \n",
      "1          0             0       7.0  \n"
     ]
    }
   ],
   "source": [
    "#  PART 3: EXTRACT DATA FOR 2014 - 2015 FIRES\n",
    "\n",
    "# Define path to the source file of fires 2014 - 2015\n",
    "file_2014 = 'Resources/CAL_FireStats/2014-wildfire-activity-stats.xlsx'\n",
    "file_2015 = 'Resources/CAL_FireStats/2015-wildfire-activity-stats.xlsx'\n",
    "# Define the sheets to be read thta contain needed information (different number of pages than 2016, 2018-2019 files)\n",
    "sheets = ['table_page_13', 'table_page_14', 'table_page_15']\n",
    "# Define the columns to be extracted\n",
    "columns_to_extract = ['County', 'Fire Name', 'Start', 'Cont.', 'Total', \n",
    "                      'Dest.', 'Dam.', 'Fire', 'Civil']\n",
    "# Function to load and process the data from the given Excel file\n",
    "def load_wildfire_data(file_path, sheets, columns_to_extract):\n",
    "    extracted_data = pd.DataFrame()\n",
    "    \n",
    "    for sheet_name in sheets:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1, usecols=columns_to_extract)\n",
    "        extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Start' and 'Cont.' columns to datetime format\n",
    "    extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
    "    extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Dest.', 'Dam.', 'Fire', and 'Civil' columns to integers and replace NaN values with zero\n",
    "    extracted_data['Dest.'] = pd.to_numeric(extracted_data['Dest.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Dam.'] = pd.to_numeric(extracted_data['Dam.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Fire'] = pd.to_numeric(extracted_data['Fire'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Civil'] = pd.to_numeric(extracted_data['Civil'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Rename columns\n",
    "    extracted_data = extracted_data.rename(columns={'Cont.': \"Contained\", 'Total': 'Acres', 'Dest.': 'Strux_Destr', 'Dam.': 'Strux_Dmgd', \n",
    "                                                    'Fire': 'Deaths_FF', 'Civil': 'Deaths_Civil'})\n",
    "    \n",
    "    # Create a new column 'Duration' that calculates the number of days between 'Start' and 'Contained'\n",
    "    extracted_data['Duration'] = (extracted_data['Contained'] - extracted_data['Start']).dt.days + 1\n",
    "\n",
    "    # Reorder columns\n",
    "    extracted_data = extracted_data[['County', 'Fire Name', 'Start', 'Contained', 'Acres', \n",
    "                                     'Strux_Destr', 'Strux_Dmgd', 'Deaths_FF', 'Deaths_Civil', 'Duration']]\n",
    "    \n",
    "    return extracted_data\n",
    "# Load data for 2014 - 2015\n",
    "fires_2014 = load_wildfire_data(file_2014, sheets, columns_to_extract)\n",
    "fires_2015 = load_wildfire_data(file_2015, sheets, columns_to_extract)\n",
    "# Display the first two rows of the data frame\n",
    "print(\"2014 Fire Data:\")\n",
    "print(fires_2014.head(2))\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"2015 Fire Data:\")\n",
    "print(fires_2015.head(2))\n",
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2014.to_csv('Outputs/2014_wildfire_data.csv', index=False)\n",
    "fires_2015.to_csv('Outputs/2015_wildfire_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 Fire Data:\n",
      "      County Fire Name      Start  Contained Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0  Riverside    PEDLEY 2010-05-12 2010-05-13   850            0           0   \n",
      "1       Kern    METZEN 2010-05-15 2010-05-15   360            1           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       2.0  \n",
      "1          0             0       1.0  \n"
     ]
    }
   ],
   "source": [
    "#  PART 4: EXTRACT DATA FOR 2010 FIRES:\n",
    "\n",
    "# Define path to the source file of fires 2010\n",
    "file_2010 = 'Resources/CAL_FireStats/2010-wildfire-activity-stats.xlsx'\n",
    "# Define the sheets to be read thta contain needed information (different number of pages than 2016, 2018-2019 files)\n",
    "sheets = ['table_page_11', 'table_page_12']\n",
    "# Define the columns to be extracted\n",
    "columns_to_extract = ['County', 'Fire Name', 'Start', 'Cont.', 'Total', \n",
    "                      'Dest.', 'Dam.', 'Fire', 'Civil']\n",
    "# Function to load and process the data from the given Excel file\n",
    "def load_wildfire_data(file_path, sheets, columns_to_extract):\n",
    "    extracted_data = pd.DataFrame()\n",
    "    \n",
    "    for sheet_name in sheets:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1, usecols=columns_to_extract)\n",
    "        extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Start' and 'Cont.' columns to datetime format\n",
    "    extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
    "    extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Dest.', 'Dam.', 'Fire', and 'Civil' columns to integers and replace NaN values with zero\n",
    "    extracted_data['Dest.'] = pd.to_numeric(extracted_data['Dest.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Dam.'] = pd.to_numeric(extracted_data['Dam.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Fire'] = pd.to_numeric(extracted_data['Fire'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Civil'] = pd.to_numeric(extracted_data['Civil'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Rename columns\n",
    "    extracted_data = extracted_data.rename(columns={'Cont.': \"Contained\", 'Total': 'Acres', 'Dest.': 'Strux_Destr', 'Dam.': 'Strux_Dmgd', \n",
    "                                                    'Fire': 'Deaths_FF', 'Civil': 'Deaths_Civil'})\n",
    "    \n",
    "    # Create a new column 'Duration' that calculates the number of days between 'Start' and 'Contained'\n",
    "    extracted_data['Duration'] = (extracted_data['Contained'] - extracted_data['Start']).dt.days + 1\n",
    "\n",
    "    # Reorder columns\n",
    "    extracted_data = extracted_data[['County', 'Fire Name', 'Start', 'Contained', 'Acres', \n",
    "                                     'Strux_Destr', 'Strux_Dmgd', 'Deaths_FF', 'Deaths_Civil', 'Duration']]\n",
    "    \n",
    "    return extracted_data\n",
    "# Load data for 2010\n",
    "fires_2010 = load_wildfire_data(file_2010, sheets, columns_to_extract)\n",
    "# Display the first two rows of the data frame\n",
    "print(\"2010 Fire Data:\")\n",
    "print(fires_2010.head(2))\n",
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2010.to_csv('Outputs/2010_wildfire_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009 Fire Data:\n",
      "           County Fire Name      Start  Contained Acres  Deaths_FF  \\\n",
      "0  San Bernardino      Fort 2009-02-05 2009-02-07   945          0   \n",
      "1      Stanislaus   Mustang 2009-05-13 2009-05-16   570          0   \n",
      "\n",
      "   Deaths_Civil  Duration  \n",
      "0             0       3.0  \n",
      "1             0       4.0  \n"
     ]
    }
   ],
   "source": [
    "#  PART 5: EXTRACT DATA FOR 2009 FIRES:\n",
    "\n",
    "# Define path to the source file of fires 2009\n",
    "file_2009 = 'Resources/CAL_FireStats/2009-wildfire-activity-stats.xlsx'\n",
    "# Define the sheets to be read thta contain needed information (different number of pages than 2016, 2018-2019 files)\n",
    "sheets = ['table_page_15', 'table_page_16', 'table_page_17', 'table_page_18']\n",
    "# Define the columns to be extracted\n",
    "columns_to_extract = ['County', 'Fire Name', 'Start', 'Cont.', 'Total', \n",
    "                        'Fire', 'Civil']\n",
    "# Function to load and process the data from the given Excel file\n",
    "def load_wildfire_data(file_path, sheets, columns_to_extract):\n",
    "    extracted_data = pd.DataFrame()\n",
    "    \n",
    "    for sheet_name in sheets:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1, usecols=columns_to_extract)\n",
    "        extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Start' and 'Cont.' columns to datetime format\n",
    "    extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
    "    extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Fire', and 'Civil' columns to integers and replace NaN values with zero\n",
    "    extracted_data['Fire'] = pd.to_numeric(extracted_data['Fire'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Civil'] = pd.to_numeric(extracted_data['Civil'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Rename columns\n",
    "    extracted_data = extracted_data.rename(columns={'Cont.': \"Contained\", 'Total': 'Acres',\n",
    "                                                    'Fire': 'Deaths_FF', 'Civil': 'Deaths_Civil'})\n",
    "    \n",
    "    # Create a new column 'Duration' that calculates the number of days between 'Start' and 'Contained'\n",
    "    extracted_data['Duration'] = (extracted_data['Contained'] - extracted_data['Start']).dt.days + 1\n",
    "\n",
    "    # Reorder columns\n",
    "    extracted_data = extracted_data[['County', 'Fire Name', 'Start', 'Contained', 'Acres', \n",
    "                                    'Deaths_FF', 'Deaths_Civil', 'Duration']]\n",
    "    \n",
    "    return extracted_data\n",
    "# Load data for 2009\n",
    "fires_2009 = load_wildfire_data(file_2009, sheets, columns_to_extract)\n",
    "# Display the first two rows of the data frame\n",
    "print(\"2009 Fire Data:\")\n",
    "print(fires_2009.head(2))\n",
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2009.to_csv('Outputs/2009_wildfire_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008 Fire Data:\n",
      "           County Fire Name      Start  Contained  Acres  Deaths_FF  \\\n",
      "0  San Bernardino     Bluff 2008-03-16 2008-03-20    680          0   \n",
      "1    Tehama-Glenn   Colyear 2008-05-06 2008-05-09  1,331          0   \n",
      "\n",
      "   Deaths_Civil  Duration  \n",
      "0             0       5.0  \n",
      "1             0       4.0  \n"
     ]
    }
   ],
   "source": [
    "#  PART 6: EXTRACT DATA FOR 2008 FIRES:\n",
    "\n",
    "# Define path to the source file of fires 2008\n",
    "file_2008 = 'Resources/CAL_FireStats/2008-wildfire-activity-stats.xlsx'\n",
    "# Define the sheets to be read thta contain needed information (different number of pages than 2016, 2018-2019 files)\n",
    "sheets = ['table_page_15', 'table_page_16', 'table_page_17', 'table_page_18', 'table_page_19', 'table_page_20', 'table_page_21', 'table_page_22']\n",
    "# Define the columns to be extracted\n",
    "columns_to_extract = ['County', 'Fire Name', 'Start', 'Cont.', 'Total', \n",
    "                        'Fire', 'Civil']\n",
    "# Function to load and process the data from the given Excel file\n",
    "def load_wildfire_data(file_path, sheets, columns_to_extract):\n",
    "    extracted_data = pd.DataFrame()\n",
    "    \n",
    "    for sheet_name in sheets:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1, usecols=columns_to_extract)\n",
    "        extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Start' and 'Cont.' columns to datetime format\n",
    "    extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
    "    extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Fire', and 'Civil' columns to integers and replace NaN values with zero\n",
    "    extracted_data['Fire'] = pd.to_numeric(extracted_data['Fire'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Civil'] = pd.to_numeric(extracted_data['Civil'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Rename columns\n",
    "    extracted_data = extracted_data.rename(columns={'Cont.': \"Contained\", 'Total': 'Acres',\n",
    "                                                    'Fire': 'Deaths_FF', 'Civil': 'Deaths_Civil'})\n",
    "    \n",
    "    # Create a new column 'Duration' that calculates the number of days between 'Start' and 'Contained'\n",
    "    extracted_data['Duration'] = (extracted_data['Contained'] - extracted_data['Start']).dt.days + 1\n",
    "\n",
    "    # Reorder columns\n",
    "    extracted_data = extracted_data[['County', 'Fire Name', 'Start', 'Contained', 'Acres', \n",
    "                                    'Deaths_FF', 'Deaths_Civil', 'Duration']]\n",
    "    \n",
    "    return extracted_data\n",
    "# Load data for 2008\n",
    "fires_2008 = load_wildfire_data(file_2008, sheets, columns_to_extract)\n",
    "# Display the first two rows of the data frame\n",
    "print(\"2008 Fire Data:\")\n",
    "print(fires_2008.head(2))\n",
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2008.to_csv('Outputs/2009_wildfire_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PART 7:  Combine the data from 2008 to 2019\n",
    "\n",
    "fires_2008_2019= pd.concat([fires_2008, fires_2009, fires_2010, fires_2014, fires_2015, fires_2017, fires_2011_2012_2013_2016_2018_2019_data], ignore_index=True)\n",
    "                           \n",
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2008_2019.to_csv('Outputs/2008_2019_wildfire_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where 'Start' is after 'Cont.':\n",
      "       County Fire Name      Start  Contained  Acres  Deaths_FF  Deaths_Civil  \\\n",
      "259  Tuolumne     SLOPE 2010-07-25 2010-01-23  1,711          0             0   \n",
      "798    TEHAMA      DALE 2028-07-09 2018-07-09    856          0             0   \n",
      "\n",
      "     Duration  Strux_Destr  Strux_Dmgd  \n",
      "259    -182.0          0.0         0.0  \n",
      "798   -3652.0          0.0         0.0  \n"
     ]
    }
   ],
   "source": [
    "# PART 8: FINAL CLEAN UP OF THE COMBINED DATA\n",
    "\n",
    "# Sort the final data by the start and then contained dates (in case of multiple fires started on the same date)\n",
    "fires_2008_2019_sorted = fires_2008_2019.sort_values(by=['Start', 'Contained'], ascending=True)\n",
    "\n",
    "# Remove rows where 'County' column is empty (NaN or empty strings) to remove summary lines from the data\n",
    "fires_2008_2019_filtered = fires_2008_2019_sorted.dropna(subset=['County']) # Remove Nan values\n",
    "fires_2008_2019_filtered = fires_2008_2019_filtered[fires_2008_2019_filtered['County'].str.strip() != ''] #Remove empty lines\n",
    "\n",
    "# Identify where 'Start' is after 'Cont.'\n",
    "incorrect_dates = fires_2008_2019_filtered[fires_2008_2019_filtered['Start'] > fires_2008_2019_filtered['Contained']]\n",
    "\n",
    "# Display the rows where 'Start' is after 'Cont.'\n",
    "if not incorrect_dates.empty:\n",
    "    print(\"Rows where 'Start' is after 'Cont.':\")\n",
    "    print(incorrect_dates)\n",
    "else:\n",
    "        print(\"No rows found where 'Start' is after 'Cont.'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Start  Contained\n",
      "259 2010-01-23 2010-08-23\n",
      "798 2017-07-09 2018-07-09\n"
     ]
    }
   ],
   "source": [
    "# The records  above indicate incorrect dates (fire can't start after 'contained' date.)\n",
    "# Correcting dates based on verified historical records:\n",
    "\n",
    "# Correcting record 798 to '2017/07/09' and record 259 to '2010-08-23'\n",
    "fires_2008_2019_filtered.at[798, 'Start'] = '2017/07/09'\n",
    "fires_2008_2019_filtered.at[259, 'Contained'] = '2010-08-23'\n",
    "\n",
    "# Ensure that the changes are in datetime format\n",
    "fires_2008_2019_filtered['Start'] = pd.to_datetime(fires_2008_2019_filtered['Start'], errors='coerce')\n",
    "fires_2008_2019_filtered['Contained'] = pd.to_datetime(fires_2008_2019_filtered['Contained'], errors='coerce')\n",
    "\n",
    "# Optional: Check if the corrections were successful\n",
    "print(fires_2008_2019_filtered.loc[[259, 798], ['Start', 'Contained']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors were found in fire duration count.\n",
      "         Start  Contained Duration\n",
      "889 2019-11-25 2019-11-25   1 days\n",
      "917 2019-11-25 2019-12-13  19 days\n",
      "99         NaT        NaT      NaT\n",
      "187        NaT        NaT      NaT\n",
      "251        NaT        NaT      NaT\n"
     ]
    }
   ],
   "source": [
    "# Sort the cleaned data by 'Start' and 'Contained' dates\n",
    "fires_2008_2019_cleaned = fires_2008_2019_filtered.sort_values(by=['Start', 'Contained'], ascending=True)\n",
    "\n",
    "# Calculate the duration of the fire (assuming 'Contained' is the end date and 'Start' is the start date)\n",
    "fires_2008_2019_cleaned['Duration'] = fires_2008_2019_cleaned['Contained'] - fires_2008_2019_cleaned['Start']\n",
    "\n",
    "# To add one day to the duration to include the start date fully, you can do this:\n",
    "fires_2008_2019_cleaned['Duration'] = fires_2008_2019_cleaned['Duration'] + pd.Timedelta(days=1)\n",
    "\n",
    "# Display the rows where 'Duration' is negative\n",
    "negative_duration = fires_2008_2019_cleaned[fires_2008_2019_cleaned['Duration'] < pd.Timedelta(0)]\n",
    "if not negative_duration.empty:\n",
    "    print(\"Duration is incorrect:\")\n",
    "    print(negative_duration)\n",
    "else:\n",
    "    print(\"No errors were found in fire duration count.\")\n",
    "\n",
    "# Print the cleaned data to verify no errors\n",
    "print(fires_2008_2019_cleaned[['Start', 'Contained', 'Duration']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows with NaT in 'Start', 'Contained', or 'Duration': 0\n",
      "         Start  Contained Duration\n",
      "885 2019-10-31 2019-11-03   4 days\n",
      "887 2019-10-31 2019-11-06   7 days\n",
      "888 2019-11-03 2019-11-06   4 days\n",
      "889 2019-11-25 2019-11-25   1 days\n",
      "917 2019-11-25 2019-12-13  19 days\n"
     ]
    }
   ],
   "source": [
    "# Display above shows summary lines appeared on our combined data\n",
    "\n",
    "# Remove rows where 'Start', 'Contained', or 'Duration' columns have NaT values\n",
    "fires_2008_2019_cleaned = fires_2008_2019_cleaned.dropna(subset=['Start', 'Contained', 'Duration'])\n",
    "\n",
    "# Check if any such rows remain\n",
    "remaining_nat_records = fires_2008_2019_cleaned[fires_2008_2019_cleaned[['Start', 'Contained', 'Duration']].isna().any(axis=1)]\n",
    "print(f\"Remaining rows with NaT in 'Start', 'Contained', or 'Duration': {len(remaining_nat_records)}\")\n",
    "\n",
    "# Print the cleaned data to verify all was corrected\n",
    "print(fires_2008_2019_cleaned[['Start', 'Contained', 'Duration']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suspiciously long durations:\n",
      "       County Fire Name      Start  Contained   Acres  Deaths_FF  \\\n",
      "146  MONTEREY     CHALK 2008-09-25 2028-10-29  16,269          0   \n",
      "259  Tuolumne     SLOPE 2010-01-23 2010-08-23   1,711          0   \n",
      "645  MARIPOSA   CASCADE 2012-06-16 2012-11-26   1,705          0   \n",
      "798    TEHAMA      DALE 2017-07-09 2018-07-09     856          0   \n",
      "860    COLUSA      SAND 2019-06-08 2020-06-15   2,220          0   \n",
      "\n",
      "     Deaths_Civil  Duration  Strux_Destr  Strux_Dmgd  \n",
      "146             0 7340 days          NaN         NaN  \n",
      "259             0  213 days          0.0         0.0  \n",
      "645             0  164 days          0.0         0.0  \n",
      "798             0  366 days          0.0         0.0  \n",
      "860             0  374 days          4.0         0.0  \n"
     ]
    }
   ],
   "source": [
    "# Verifying if there are any other errors in the data (like suspiciously long fires)\n",
    "\n",
    "# Define a timedelta representing 150 days\n",
    "threshold_duration = pd.Timedelta(days=150)\n",
    "# Find a d display if any fires lasted longer than 150 days according to our data\n",
    "long_durations = fires_2008_2019_cleaned[fires_2008_2019_cleaned['Duration'] > threshold_duration]\n",
    "print(\"Suspiciously long durations:\")\n",
    "print(long_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected record:\n",
      "County                       MERCED\n",
      "Fire Name                     CREEK\n",
      "Start           2015-07-31 00:00:00\n",
      "Contained       2019-07-31 00:00:00\n",
      "Acres                         1,450\n",
      "Deaths_FF                         0\n",
      "Deaths_Civil                      0\n",
      "Duration            1 days 00:00:00\n",
      "Strux_Destr                     0.0\n",
      "Strux_Dmgd                      0.0\n",
      "Name: 340, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# According to Google SAND fired was contained on 2019-06-15, 1 year earlier than our data shows.\n",
    "# Correct the year in the 'Contained' column to 2019\n",
    "fires_2008_2019_cleaned.loc[340, 'Contained'] = fires_2008_2019_cleaned.loc[340, 'Contained'].replace(year=2019)\n",
    "# Verify the updated record\n",
    "print(\"Corrected record:\")\n",
    "print(fires_2008_2019_cleaned.loc[340])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timedelta64[ns]\n",
      "Updated record with recalculated Duration:\n",
      "County                       MERCED\n",
      "Fire Name                     CREEK\n",
      "Start           2015-07-31 00:00:00\n",
      "Contained       2019-07-31 00:00:00\n",
      "Acres                         1,450\n",
      "Deaths_FF                         0\n",
      "Deaths_Civil                      0\n",
      "Duration         1462 days 00:00:00\n",
      "Strux_Destr                     0.0\n",
      "Strux_Dmgd                      0.0\n",
      "Name: 340, dtype: object\n",
      "timedelta64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'Duration' column to timedelta if it is not already\n",
    "fires_2008_2019_cleaned['Duration'] = pd.to_timedelta(fires_2008_2019_cleaned['Duration'], errors='coerce')\n",
    "\n",
    "# Check the dtype to verify the conversion\n",
    "print(fires_2008_2019_cleaned['Duration'].dtype)\n",
    "\n",
    "# Recalculate the 'Duration' for the corrected record\n",
    "duration = fires_2008_2019_cleaned.loc[340, 'Contained'] - fires_2008_2019_cleaned.loc[340, 'Start'] + pd.Timedelta(days=1)\n",
    "\n",
    "# Update the 'Duration' column with the recalculated duration\n",
    "fires_2008_2019_cleaned.at[340, 'Duration'] = duration\n",
    "\n",
    "# Print the updated record to verify the recalculated 'Duration'\n",
    "print(\"Updated record with recalculated Duration:\")\n",
    "print(fires_2008_2019_cleaned.loc[340])\n",
    "print(fires_2008_2019_cleaned['Duration'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             County              Fire Name      Start  Contained  Acres  \\\n",
      "100            LAKE  CONTROL BURN, GEYSERS 2008-02-13 2008-02-13    400   \n",
      "0    San Bernardino                  Bluff 2008-03-16 2008-03-20    680   \n",
      "101        MARIPOSA              WAWONA NW 2008-04-09 2008-04-19  1,130   \n",
      "102     LOS ANGELES            SANTA ANITA 2008-04-26 2008-05-02    584   \n",
      "103       RIVERSIDE                 APACHE 2008-04-29 2008-05-04    769   \n",
      "\n",
      "     Deaths_FF  Deaths_Civil Duration  Strux_Destr  Strux_Dmgd  Duration_Days  \n",
      "100          0             0   1 days          NaN         NaN              1  \n",
      "0            0             0   5 days          NaN         NaN              5  \n",
      "101          0             0  11 days          NaN         NaN             11  \n",
      "102          0             0   7 days          NaN         NaN              7  \n",
      "103          0             0   6 days          NaN         NaN              6  \n"
     ]
    }
   ],
   "source": [
    "# Convert 'Duration' to timedelta format\n",
    "fires_2008_2019_cleaned['Duration'] = pd.to_timedelta(fires_2008_2019_cleaned['Duration'])\n",
    "\n",
    "# Calculate the duration in days and convert it to an integer\n",
    "fires_2008_2019_cleaned['Duration_Days'] = fires_2008_2019_cleaned['Duration'].dt.days\n",
    "\n",
    "# If you want to format the 'Duration' as a string like 'X days HH:MM:SS'\n",
    "# fires_2015_2019_cleaned['Duration_Str'] = fires_2015_2019_cleaned['Duration'].apply(\n",
    "    #lambda x: f\"{x.days} days {x.components.hours:02}:{x.components.minutes:02}:{x.components.seconds:02}\"\n",
    "#)\n",
    "\n",
    "# Print the cleaned data to check results\n",
    "print(fires_2008_2019_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counties that do not match the valid list:\n",
      "               County\n",
      "1        TEHAMA-GLENN\n",
      "2        FRESNO-KINGS\n",
      "3        FRESNO-KINGS\n",
      "6    SONOMA-LAKE-NAPA\n",
      "9    MARDERA-MARIPOSA\n",
      "..                ...\n",
      "600      LAKE/ COLUSA\n",
      "599       LAKE/COLUSA\n",
      "604   TEHAMA/\\nSHASTA\n",
      "467            WASHOE\n",
      "493            WASHOE\n",
      "\n",
      "[71 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check if any county are misspelled \n",
    "# List of valid county names\n",
    "valid_counties = [\n",
    "    'ALAMEDA', 'ALPINE', 'AMADOR', 'BUTTE', 'CALAVERAS', 'COLUSA', 'CONTRA COSTA', \n",
    "    'DEL NORTE', 'EL DORADO', 'FRESNO', 'GLENN', 'HUMBOLDT', 'IMPERIAL', 'INYO', \n",
    "    'KERN', 'KINGS', 'LAKE', 'LASSEN', 'LOS ANGELES', 'MADERA', 'MARIN', 'MARIPOSA', \n",
    "    'MENDOCINO', 'MERCED', 'MODOC', 'MONO', 'MONTEREY', 'NAPA', 'NEVADA', 'ORANGE', \n",
    "    'PLACER', 'PLUMAS', 'RIVERSIDE', 'SACRAMENTO', 'SAN BENITO', 'SAN BERNARDINO', \n",
    "    'SAN DIEGO', 'SAN FRANCISCO', 'SAN JOAQUIN', 'SAN LUIS OBISPO', 'SAN MATEO', \n",
    "    'SANTA BARBARA', 'SANTA CLARA', 'SANTA CRUZ', 'SHASTA', 'SIERRA', 'SISKIYOU', \n",
    "    'SOLANO', 'SONOMA', 'STANISLAUS', 'SUTTER', 'TEHAMA', 'TRINITY', 'TULARE', \n",
    "    'TUOLUMNE', 'VENTURA', 'YOLO', 'YUBA', 'JACKSON (OR)', 'WASHOE (NV)'\n",
    "]\n",
    "# Convert the 'County' column to uppercase to ensure matching\n",
    "fires_2008_2019_cleaned['County'] = fires_2008_2019_cleaned['County'].str.upper()\n",
    "\n",
    "# Find counties that are not in the valid counties list\n",
    "invalid_counties = fires_2008_2019_cleaned[~fires_2008_2019_cleaned['County'].isin(valid_counties)]\n",
    "\n",
    "# Display the rows with invalid counties\n",
    "print(\"Counties that do not match the valid list:\")\n",
    "print(invalid_counties[['County']])\n",
    "# Saving incorrect list of misspelled counties for review\n",
    "invalid_counties.to_csv('Outputs/invalid_counties.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 71 rows were not identified correctly (please see the output above)\n",
    "\n",
    "# Define a function to clean up the 'County' column\n",
    "def clean_county_name(county_name):\n",
    "    # Remove text after '-', ',', and '/' and strip any extra whitespace\n",
    "    return county_name.split('-')[0].split(',')[0].split('/')[0].split('\\\\')[0].split('\\\\n')[0].strip()\n",
    "\n",
    "# Apply the cleaning function to the 'County' column\n",
    "fires_2008_2019_cleaned['County'] = fires_2008_2019_cleaned['County'].apply(clean_county_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid counties found:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Continue with your corrections dictionary for specific misspellings\n",
    "corrections = {\n",
    "    'TAHEMA': 'TEHAMA',\n",
    "    'ELDORADO': 'EL DORADO',\n",
    "    'TEHEMA': 'TEHAMA',\n",
    "    'TOULUMNE': 'TUOLUMNE',\n",
    "    'WASHOE': 'WASHOE (NV)',\n",
    "    'VANDENBURG AFB': 'SANTA BARBARA',\n",
    "    'MARDERA': 'MADERA',\n",
    "    'VENTURA/SANTA\\nBARBARA': 'VENTURA',\n",
    "    'COLUSA, GLENN,\\nLAKE, MENDOCINO': 'COLUSA',\n",
    "    'COLUSA, LAKE,\\nMENDOCINO': 'COLUSA',\n",
    "    'SANTA\\nBARBARA': 'SANTA BARBARA',\n",
    "    'SAN LUIS\\nOBISPO': 'SAN LUIS OBISPO',\n",
    "    'SAN\\nBERNARDINO': 'SAN BERNARDINO',\n",
    "    'VANDENBURG AFB': 'SANTA BARBARA'\n",
    "}\n",
    "\n",
    "# Apply the corrections to the 'County' column\n",
    "fires_2008_2019_cleaned['County'] = fires_2008_2019_cleaned['County'].replace(corrections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counties that do not match the valid list:\n",
      "Empty DataFrame\n",
      "Columns: [County]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find counties that are not in the valid counties list\n",
    "invalid_counties_test = fires_2008_2019_cleaned[~fires_2008_2019_cleaned['County'].isin(valid_counties)]\n",
    "\n",
    "# Display the rows with invalid counties\n",
    "print(\"Counties that do not match the valid list:\")\n",
    "print(invalid_counties_test[['County']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            County  Fire Name      Start  Contained  Acres  Deaths_FF  \\\n",
      "885      RIVERSIDE       HILL 2019-10-31 2019-11-03    628          0   \n",
      "887        VENTURA      MARIA 2019-10-31 2019-11-06  9,999          0   \n",
      "888         TEHAMA      RANCH 2019-11-03 2019-11-06  2,534          0   \n",
      "889         PLACER  FOOTHILLS 2019-11-25 2019-11-25    308          0   \n",
      "917  SANTA BARBARA       CAVE 2019-11-25 2019-12-13  3,126          0   \n",
      "\n",
      "     Deaths_Civil Duration  Strux_Destr  Strux_Dmgd  Duration_Days  \n",
      "885             0   4 days          0.0         0.0              4  \n",
      "887             0   7 days          4.0         0.0              7  \n",
      "888             0   4 days          0.0         1.0              4  \n",
      "889             0   1 days          0.0         0.0              1  \n",
      "917             0  19 days          0.0         1.0             19  \n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned data to the \"Outputs\" folder\n",
    "fires_2008_2019_cleaned.to_csv('Outputs/fires_2008_2019_cleaned.csv', index=False)\n",
    "\n",
    "# Print the cleaned data to check\n",
    "print(fires_2008_2019_cleaned.tail(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
