{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAERT 1:  EXTRACT DATA FOR 2016, 2018, and 2019 FIRES \n",
    "\n",
    "# Define path to the source files\n",
    "file_2016 = 'Resources/CAL_FireStats/2016-wildfire-activity-stats.xlsx'\n",
    "file_2018 = 'Resources/CAL_FireStats/2018-wildfire-activity-stats.xlsx'\n",
    "file_2019 = 'Resources/CAL_FireStats/2019-wildfire-activity-stats.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sheets to be read (all these files contain needed information on the same pages)\n",
    "sheets = ['table_page_13', 'table_page_14', 'table_page_15', 'table_page_16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be extracted\n",
    "columns_to_extract = ['County', 'Fire Name', 'Start', 'Cont.', 'Total', \n",
    "                      'Dest.', 'Dam.', 'Fire', 'Civil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and process the data from the given Excel file\n",
    "def load_wildfire_data(file_path, sheets, columns_to_extract):\n",
    "    # Initiate an empty Data Frame\n",
    "    extracted_data = pd.DataFrame()\n",
    "    # Set a 'for' loop to go through identified path, sheets, and columns\n",
    "    # skip the 1st row as it does not contain iseful information\n",
    "    for sheet_name in sheets:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1, usecols=columns_to_extract)\n",
    "        # Add the data to an existing collection of data, Index=True to re-number the rows after adding a new data\n",
    "        extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Start' and 'Cont.' columns to datetime format\n",
    "    extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
    "    extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Dest.', 'Dam.', 'Fire', and 'Civil' columns to integers and replace NaN values with zero\n",
    "    extracted_data['Dest.'] = pd.to_numeric(extracted_data['Dest.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Dam.'] = pd.to_numeric(extracted_data['Dam.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Fire'] = pd.to_numeric(extracted_data['Fire'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Civil'] = pd.to_numeric(extracted_data['Civil'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Rename columns\n",
    "    extracted_data = extracted_data.rename(columns={'Cont.': \"Contained\", 'Total': 'Acres', 'Dest.': 'Strux_Destr', 'Dam.': 'Strux_Dmgd', \n",
    "                                                    'Fire': 'Deaths_FF', 'Civil': 'Deaths_Civil'})\n",
    "    \n",
    "    # Create a new column 'Duration' that calculates the number of days between 'Start' and 'Contained'\n",
    "    extracted_data['Duration'] = (extracted_data['Contained'] - extracted_data['Start']).dt.days + 1\n",
    "\n",
    "    # Reorder columns\n",
    "    extracted_data = extracted_data[['County', 'Fire Name', 'Start', 'Contained', 'Acres', \n",
    "                                     'Strux_Destr', 'Strux_Dmgd', 'Deaths_FF', 'Deaths_Civil', 'Duration']]\n",
    "    # Return the processed data as a DataFrame so it can be used outside the function\n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\1362880317.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\1362880317.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\1362880317.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\1362880317.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\1362880317.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\1362880317.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n"
     ]
    }
   ],
   "source": [
    "# Load data for 2016, 2018, 2019 years\n",
    "fires_2016 = load_wildfire_data(file_2016, sheets, columns_to_extract)\n",
    "fires_2018 = load_wildfire_data(file_2018, sheets, columns_to_extract)\n",
    "fires_2019 = load_wildfire_data(file_2019, sheets, columns_to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data from 2016, 2018, and 2019 into one DataFrame\n",
    "fires_2016_2018_2019_data = pd.concat([fires_2016, fires_2018, fires_2019], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 Fire Data:\n",
      "      County Fire Name      Start  Contained  Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0   MONTEREY      METZ 2016-05-22 2016-05-25  3,876            0           0   \n",
      "1  SAN DIEGO  BORDER 3 2016-06-19 2016-07-01  7,609           16           3   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       4.0  \n",
      "1          0             2      13.0  \n",
      "--------------------------------------\n",
      "\n",
      "2018 Fire Data:\n",
      "      County  Fire Name      Start  Contained  Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0       INYO     MOFFAT 2018-04-19 2018-04-24  1,065            0           0   \n",
      "1     MERCED       NEES 2018-05-02 2018-05-02  1,756            0           0   \n",
      "2  RIVERSIDE  PATTERSON 2018-05-17 2018-05-18  1,261            0           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       6.0  \n",
      "1          0             0       1.0  \n",
      "2          0             0       2.0  \n",
      "--------------------------------------\n",
      "\n",
      "2019 Fire Data:\n",
      "            County Fire Name      Start  Contained  Acres  Strux_Destr  \\\n",
      "0        RIVERSIDE   LINCOLN 2019-03-15 2019-03-20    560            0   \n",
      "1  SAN LUIS OBISPO   BELMONT 2019-05-29 2019-05-29    835            0   \n",
      "2  SAN LUIS OBISPO   BOULDER 2019-06-05 2019-06-05  1,075            0   \n",
      "\n",
      "   Strux_Dmgd  Deaths_FF  Deaths_Civil  Duration  \n",
      "0           0          0             0       6.0  \n",
      "1           0          0             0       1.0  \n",
      "2           0          0             0       1.0  \n",
      "--------------------------------------\n",
      "\n",
      "Combined Fire Data (2016, 2018, 2019):\n",
      "      County Fire Name      Start  Contained  Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0   MONTEREY      METZ 2016-05-22 2016-05-25  3,876            0           0   \n",
      "1  SAN DIEGO  BORDER 3 2016-06-19 2016-07-01  7,609           16           3   \n",
      "2     MERCED  DINOSAUR 2016-06-25 2016-06-26  1,246            0           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       4.0  \n",
      "1          0             2      13.0  \n",
      "2          0             0       2.0  \n"
     ]
    }
   ],
   "source": [
    "# Display the first two rows of the 2016, 2018, 2019, and combined dataframes to check the results\n",
    "print(\"2016 Fire Data:\")\n",
    "print(fires_2016.head(2))\n",
    "print(\"--------------------------------------\")\n",
    "print(\"\\n2018 Fire Data:\")\n",
    "print(fires_2018.head(3))\n",
    "print(\"--------------------------------------\")\n",
    "print(\"\\n2019 Fire Data:\")\n",
    "print(fires_2019.head(3))\n",
    "print(\"--------------------------------------\")\n",
    "print(\"\\nCombined Fire Data (2016, 2018, 2019):\")\n",
    "print(fires_2016_2018_2019_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2016.to_csv('Outputs/2016_wildfire_data.csv', index=False)\n",
    "fires_2018.to_csv('Outputs/2018_wildfire_data.csv', index=False)\n",
    "fires_2019.to_csv('Outputs/2019_wildfire_data.csv', index=False)\n",
    "fires_2016_2018_2019_data.to_csv('Outputs/2016_2018_2019_wildfire_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Fire Data:\n",
      "   County  Fire Name      Start  Contained  Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0  FRESNO      JAYNE 2017-04-20 2017-04-21  4,532            0           0   \n",
      "1  FRESNO  EL DORADO 2017-04-28 2017-04-28    750            0           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       2.0  \n",
      "1          0             0       1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\3323295070.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\3323295070.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n"
     ]
    }
   ],
   "source": [
    "#  PART 2: EXTRACT DATA FOR 2017 FIRES:\n",
    "\n",
    "# Define path to the source file of fires 2017\n",
    "file_2017 = 'Resources/CAL_FireStats/2017-wildfire-activity-stats.xlsx'\n",
    "# Define the sheets to be read thta contain needed information (different number of pages than 2016, 2018-2019 files)\n",
    "sheets = ['table_page_13', 'table_page_14', 'table_page_15', 'table_page_16', 'table_page_17', 'table_page_18']\n",
    "# Define the columns to be extracted\n",
    "columns_to_extract = ['County', 'Fire Name', 'Start', 'Cont.', 'Total', \n",
    "                      'Dest.', 'Dam.', 'Fire', 'Civil']\n",
    "# Function to load and process the data from the given Excel file\n",
    "def load_wildfire_data(file_path, sheets, columns_to_extract):\n",
    "    extracted_data = pd.DataFrame()\n",
    "    \n",
    "    for sheet_name in sheets:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1, usecols=columns_to_extract)\n",
    "        extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Start' and 'Cont.' columns to datetime format\n",
    "    extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
    "    extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Dest.', 'Dam.', 'Fire', and 'Civil' columns to integers and replace NaN values with zero\n",
    "    extracted_data['Dest.'] = pd.to_numeric(extracted_data['Dest.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Dam.'] = pd.to_numeric(extracted_data['Dam.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Fire'] = pd.to_numeric(extracted_data['Fire'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Civil'] = pd.to_numeric(extracted_data['Civil'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Rename columns\n",
    "    extracted_data = extracted_data.rename(columns={'Cont.': \"Contained\", 'Total': 'Acres', 'Dest.': 'Strux_Destr', 'Dam.': 'Strux_Dmgd', \n",
    "                                                    'Fire': 'Deaths_FF', 'Civil': 'Deaths_Civil'})\n",
    "    \n",
    "    # Create a new column 'Duration' that calculates the number of days between 'Start' and 'Contained'\n",
    "    extracted_data['Duration'] = (extracted_data['Contained'] - extracted_data['Start']).dt.days + 1\n",
    "\n",
    "    # Reorder columns\n",
    "    extracted_data = extracted_data[['County', 'Fire Name', 'Start', 'Contained', 'Acres', \n",
    "                                     'Strux_Destr', 'Strux_Dmgd', 'Deaths_FF', 'Deaths_Civil', 'Duration']]\n",
    "    \n",
    "    return extracted_data\n",
    "# Load data for 2017\n",
    "fires_2017 = load_wildfire_data(file_2017, sheets, columns_to_extract)\n",
    "# Display the first two rows of the data frame\n",
    "print(\"2017 Fire Data:\")\n",
    "print(fires_2017.head(2))\n",
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2017.to_csv('Outputs/2017_wildfire_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 Fire Data:\n",
      "      County Fire Name      Start  Contained  Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "0       INYO     ROUND 2015-02-06 2015-02-13  7,000           43           5   \n",
      "1  RIVERSIDE   HIGHWAY 2015-04-18 2015-04-24  1,049            0           0   \n",
      "\n",
      "   Deaths_FF  Deaths_Civil  Duration  \n",
      "0          0             0       8.0  \n",
      "1          0             0       7.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\819243619.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
      "C:\\Users\\mssab\\AppData\\Local\\Temp\\ipykernel_28676\\819243619.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n"
     ]
    }
   ],
   "source": [
    "#  PART 3: EXTRACT DATA FOR 2015 FIRES\n",
    "\n",
    "# Define path to the source file of fires 2015\n",
    "file_2015 = 'Resources/CAL_FireStats/2015-wildfire-activity-stats.xlsx'\n",
    "# Define the sheets to be read thta contain needed information (different number of pages than 2016, 2018-2019 files)\n",
    "sheets = ['table_page_13', 'table_page_14', 'table_page_15']\n",
    "# Define the columns to be extracted\n",
    "columns_to_extract = ['County', 'Fire Name', 'Start', 'Cont.', 'Total', \n",
    "                      'Dest.', 'Dam.', 'Fire', 'Civil']\n",
    "# Function to load and process the data from the given Excel file\n",
    "def load_wildfire_data(file_path, sheets, columns_to_extract):\n",
    "    extracted_data = pd.DataFrame()\n",
    "    \n",
    "    for sheet_name in sheets:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1, usecols=columns_to_extract)\n",
    "        extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Start' and 'Cont.' columns to datetime format\n",
    "    extracted_data['Start'] = pd.to_datetime(extracted_data['Start'])\n",
    "    extracted_data['Cont.'] = pd.to_datetime(extracted_data['Cont.'])\n",
    "    \n",
    "    # 'TRANSFORM': Convert 'Dest.', 'Dam.', 'Fire', and 'Civil' columns to integers and replace NaN values with zero\n",
    "    extracted_data['Dest.'] = pd.to_numeric(extracted_data['Dest.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Dam.'] = pd.to_numeric(extracted_data['Dam.'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Fire'] = pd.to_numeric(extracted_data['Fire'], errors='coerce').fillna(0).astype(int)\n",
    "    extracted_data['Civil'] = pd.to_numeric(extracted_data['Civil'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Rename columns\n",
    "    extracted_data = extracted_data.rename(columns={'Cont.': \"Contained\", 'Total': 'Acres', 'Dest.': 'Strux_Destr', 'Dam.': 'Strux_Dmgd', \n",
    "                                                    'Fire': 'Deaths_FF', 'Civil': 'Deaths_Civil'})\n",
    "    \n",
    "    # Create a new column 'Duration' that calculates the number of days between 'Start' and 'Contained'\n",
    "    extracted_data['Duration'] = (extracted_data['Contained'] - extracted_data['Start']).dt.days + 1\n",
    "\n",
    "    # Reorder columns\n",
    "    extracted_data = extracted_data[['County', 'Fire Name', 'Start', 'Contained', 'Acres', \n",
    "                                     'Strux_Destr', 'Strux_Dmgd', 'Deaths_FF', 'Deaths_Civil', 'Duration']]\n",
    "    \n",
    "    return extracted_data\n",
    "# Load data for 2017\n",
    "fires_2015 = load_wildfire_data(file_2015, sheets, columns_to_extract)\n",
    "# Display the first two rows of the data frame\n",
    "print(\"2015 Fire Data:\")\n",
    "print(fires_2015.head(2))\n",
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2015.to_csv('Outputs/2015_wildfire_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PART 4:  Combine the data from 2016, 2018, and 2019 with data from 2015, and 2017\n",
    "\n",
    "fires_2015_2019= pd.concat([fires_2015, fires_2017, fires_2016_2018_2019_data], ignore_index=True)\n",
    "                           \n",
    "# Save Data Frames to 'Outputs' folder\n",
    "fires_2015_2019.to_csv('Outputs/2015_2019_wildfire_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       County Fire Name      Start  Contained  Acres  Strux_Destr  Strux_Dmgd  \\\n",
      "26       MONO  VAN DYKE 2015-02-06 2015-02-10    509            0           0   \n",
      "0        INYO     ROUND 2015-02-06 2015-02-13  7,000           43           5   \n",
      "1   RIVERSIDE   HIGHWAY 2015-04-18 2015-04-24  1,049            0           0   \n",
      "27  SAN DIEGO      CARL 2015-04-28 2015-04-29  4,000            0           0   \n",
      "28  SAN DIEGO    MORTAR 2015-04-28 2015-04-29    800            0           0   \n",
      "\n",
      "    Deaths_FF  Deaths_Civil  Duration  \n",
      "26          0             0       5.0  \n",
      "0           0             0       8.0  \n",
      "1           0             0       7.0  \n",
      "27          0             0       2.0  \n",
      "28          0             0       2.0  \n"
     ]
    }
   ],
   "source": [
    "# Sort the final data by the start and then contained dates (in case of multiple fires started on the same date)\n",
    "fires_2015_2019_sorted = fires_2015_2019.sort_values(by=['Start', 'Contained'], ascending=True)\n",
    "\n",
    "# Remove rows where 'County' column is empty (NaN or empty strings) to remove summary lines from the data\n",
    "fires_2015_2019_filtered = fires_2015_2019_sorted.dropna(subset=['County']) # Remove Nan values\n",
    "fires_2015_2019_filtered = fires_2015_2019_filtered[fires_2015_2019_filtered['County'].str.strip() != ''] #Remove empty lines\n",
    "\n",
    "# Assign the cleaned data to a new variable\n",
    "fires_2015_2019_cleaned = fires_2015_2019_filtered\n",
    "\n",
    "# Save a cleaned data to \"Outputs\" folder\n",
    "fires_2015_2019_cleaned.to_csv('Outputs/fires_2015_2019_cleaned.csv', index=False)\n",
    "\n",
    "# Optionally, print the cleaned data to check\n",
    "print(fires_2015_2019_cleaned.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
